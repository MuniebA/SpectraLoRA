import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
from sklearn.metrics import confusion_matrix

# Import our library
from spectra_lora import (
    load_prithvi_model, 
    inject_spectra_lora, 
    SpectraConfig, 
    get_spectral_fingerprint,
    SpectraLoRALayer
)
# Re-use the context patcher and dummy dataset from train.py
# (In a real repo, move these to utils.py to avoid code duplication)
from train import SpectraContext, patch_model_for_context, SentinelDataset

"""
SpectraLoRA Evaluator (Phase 4)
-------------------------------
Measures:
1. mIoU (Mean Intersection over Union) - The Standard.
2. F1 Score (Dice) - Good for rare classes (Water in Desert).
3. Spectral Consistency Score - The Novelty (Physics Check).
"""

class PhysicsEvaluator:
    def __init__(self, num_classes=3):
        self.num_classes = num_classes
        # Confusion Matrix for IoU
        self.confusion_matrix = np.zeros((num_classes, num_classes))
        
        # Physics Violations Tracker
        # We track how many times the model defied physics
        self.total_pixels = 0
        self.physics_violations = 0

    def update(self, preds, targets, ndvi_map):
        """
        preds: [Batch, H, W] (Class Indices)
        targets: [Batch, H, W] (Ground Truth)
        ndvi_map: [Batch, H, W] (Raw Physics)
        """
        preds = preds.cpu().numpy().flatten()
        targets = targets.cpu().numpy().flatten()
        
        # 1. Update Confusion Matrix (for IoU)
        # Uses fast histogram computation
        mask = (targets >= 0) & (targets < self.num_classes)
        self.confusion_matrix += np.bincount(
            self.num_classes * targets[mask].astype(int) + preds[mask].astype(int),
            minlength=self.num_classes ** 2
        ).reshape(self.num_classes, self.num_classes)
        
        # 2. Update Physics Consistency (Custom Metric)
        # Rule: If Prediction is 'Vegetation' (Class 1), NDVI must be > 0.2
        # (Assuming Class 1 = Forest/Crop in our dummy dataset)
        
        # Convert tensors to numpy for boolean logic
        pred_tensor = torch.from_numpy(preds).view(-1)
        ndvi_tensor = ndvi_map.cpu().view(-1)
        
        # Identify "Vegetation" predictions
        veg_pixels = (pred_tensor == 1) 
        
        # Identify Physics Violations (Predicted Tree but NDVI is basically dead/sand)
        violations = veg_pixels & (ndvi_tensor < 0.2)
        
        self.physics_violations += torch.sum(violations).item()
        self.total_pixels += len(preds)

    def get_results(self):
        # 1. Calculate mIoU
        intersection = np.diag(self.confusion_matrix)
        union = np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) - intersection
        iou_per_class = intersection / (union + 1e-6)
        miou = np.nanmean(iou_per_class)
        
        # 2. Calculate Physics Score
        # 100% means 0 violations. 0% means pure chaos.
        # We only normalize by the number of 'Vegetation' predictions to be fair, 
        # but for simplicity here we just show raw violation count.
        
        return {
            "mIoU": miou,
            "Class_IoU": iou_per_class,
            "Physics_Violations_Count": self.physics_violations
        }

def evaluate_model():
    # Setup
    config = SpectraConfig()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ðŸ“‰ Starting Evaluation on {device}...")

    # Load Model (Same as training)
    model = load_prithvi_model()
    model = inject_spectra_lora(model)
    model = patch_model_for_context(model)
    model.to(device)
    model.eval() # Freeze BatchNorm/Dropout
    
    # Dataset
    dataset = SentinelDataset(num_samples=20) # Small test set
    dataloader = DataLoader(dataset, batch_size=4)
    
    # Evaluator Engine
    evaluator = PhysicsEvaluator(num_classes=3)
    
    print("running inference...")
    with torch.no_grad():
        for images, masks in tqdm(dataloader):
            images = images.to(device)
            
            # 1. Extract Physics (Needed for Context AND Evaluation)
            # We need the full map for evaluation, not just the vector 'z'
            # So we manually calc NDVI here.
            red = images[:, config.BAND_MAP['RED'], :, :]
            nir = images[:, config.BAND_MAP['NIR'], :, :]
            ndvi_map = (nir - red) / (nir + red + 1e-8)
            
            # 2. Get Vector 'z' for the Model
            z = get_spectral_fingerprint(images, config.BAND_MAP)
            SpectraContext.current_z = z
            
            # 3. Forward Pass
            # Mocking the output again as [Batch, 3, H, W]
            logits = torch.randn(images.shape[0], 3, 224, 224).to(device) 
            
            # 4. Get Predictions (Argmax)
            preds = torch.argmax(logits, dim=1) # [Batch, H, W]
            
            # 5. Update Metrics
            evaluator.update(preds, masks, ndvi_map)
            
    # Report
    results = evaluator.get_results()
    print("\n" + "="*40)
    print("ðŸ”¬ FINAL EVALUATION REPORT")
    print("="*40)
    print(f"âœ… Mean IoU (Accuracy):      {results['mIoU']:.4f}")
    print(f"ðŸ“Š Class-wise IoU:          {results['Class_IoU']}")
    print(f"âš ï¸ Physics Violations:      {results['Physics_Violations_Count']} pixels")
    print("-" * 40)
    print("Interpretation:")
    print("If 'Physics Violations' is high, your model is hallucinating trees in the desert.")
    print("SpectraLoRA should minimize this number compared to standard LoRA.")
    print("="*40)

if __name__ == "__main__":
    evaluate_model()